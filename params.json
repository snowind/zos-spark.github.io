{"name":"zos-spark.github.io","tagline":"Ecosystem of Tools for the IBM z/OS Platform for Apache Spark","body":"# zos-spark\r\nWelcome to the dedicated GitHub organization comprised of community contributions around the *IBM zOS Platform for Apache Spark*.\r\n\r\nThe intent of this project is to enable the development of an ecosystem of tools associated with a **reference architecture** that demonstrates how the *IBM zOS Platform for Apache Spark* can be combined with a variety of open source tools. Of specific interest is the combination of [Project Jupyter](http://jupyter.org) and any NoSQL database to provide a <font color=\"purple\">*flexible and extendible*</font> data processing and analytics solution.\r\n\r\n![Reference-Architecture-Diagram](https://ibm.box.com/shared/static/q322rz3364gqnmmr6ajontiv3aarj218.png)\r\n\r\n## Overview \r\n\r\n### Architecture Concepts\r\n\r\n1. *<font color=\"blue\">Data Access</font>*: At the center of the solution is a NoSQL database that is kept fresh with datasets from various data processing end-points and it serves as the central data source for analytical activity. \r\n2. *<font color=\"blue\">Flexibility</font>*: A variety of use cases can be addressed by this solution. JSON datasets can be created using Spark jobs that handle complex data munging tasks and/or Data Scientists can perform ETL activities within notebooks across multiple datasets in the NoSQL database. Depending on skills and the requirements of a particular analytical task, users can determine when and where to preform ETL activities. \r\n3. *<font color=\"blue\">Extensibility</font>*: New datasets can be injected into the NoSQL database from a variety of external datasources and/or data processing environments. For example, a separate instance of Apache Spark may be installed for use with a set of databases on distributed systems. Furthermore, users may want to run jobs that aggregate data from multiple data processing environments.  \r\n4. *<font color=\"blue\">Interoperability</font>*: An ecosystem of analytical tools are appropriately positioned to interoperate with the various solution components. For instance, the *Scala Workbench* is tightly-coupled with the installation of *IBM zOS Platform for Apache Spark*.\r\n5. *<font color=\"blue\">Integration</font>*: This solution should be viewed as a complimentary approach to the data processing challenges of an enterprise. Nothing in this solution alters the procedures, policies or existing programmatic approaches currently deployed. \r\n\r\n### Architecture Considerations\r\n\r\n1. *<font color=\"blue\">Managed Data Processing Environment</font>*: Spark provides distributed task dispatching and scheduling. It can be used to allow data processing tasks to be submitted as batch jobs on some predefined frequency. Few users need to interact with such a managed environment for data processing jobs. Only Data Wranglers need a deep understanding of Spark and the tools necessary to integrate with it.\r\n2. *<font color=\"blue\">Enterprise Analytics Repository</font>*: As data processing tasks are completed, the results of those jobs can be stored in a central location that is more easily accessible by a broader user community. New data sets that are produced by the Spark jobs can be refreshed or purged as desired by the system administrators or user community. Two obvious deployment topologies standout:\r\n\r\n\t* NoSQL database deployed in a Linux on Z partition\r\n\t* NoSQL database deployed on a distributed server \r\n\t\r\n\tIn either case, the NoSQL database chosen should sport a robust set of Python and/or R libraries for manipulating data.  \r\n3. *<font color=\"blue\">Content Format</font>*: Given the various programming languages used by Data Scientists, the Enterprise Analytics Repository should embrace a data storage format, such as JSON, that is commonly supported across programming languages and data stores.\r\n\r\n#### Solution Persona\r\n\r\n| Persona | Tool | Description |\r\n| --- | --- | --- |\r\n| Operations Data Wrangler | Spark Job Manager | Combine [Spark Job Server](https://github.com/spark-jobserver/spark-jobserver) and [Spark Job Server Client](https://github.com/bluebreezecf/SparkJobServerClient) to provide a robust toolset for [managing the lifecycle of Spark Jobs](http://www.slideshare.net/EvanChan2/productionizing-spark-and-the-spark-job-server). |\r\n| Information Management Specialist | Scala Workbench | Tightly-coupled Jupyter+Spark workbench that allows <font color=\"green\">Scala</font> users with direct access to transactional systems to query, analyze and visualize enterprise data.  |\r\n| Data Scientist | Interactive Insights Workbench | Open source tool for Python and R users to access datasets for analysis and insight generation.|\r\n\r\n#### Solution Components\r\n\r\n| Component | Purpose |\r\n| --- | --- | \r\n| IBM zOS Platform for Apache Spark | An optimized Apache Spark offering that incorporates a Spark Cluster with specialized technology from [Rocket Software](http://www.rocketsoftware.com/solutions). |\r\n| Enterprise Analytics Repository | NoSQL database that stores the result of Spark Jobs in JSON format. In this solution JSON is positioned as the common point of exchange for all languages to load and manipulate. |\r\n| Spark Job Server | Provides a REST API for submitting, running and monitoring Spark Jobs. Also enables the results of jobs to be converted to JSON format. |\r\n| Spark Job Server Client | Java Client API for development of client GUI tools that will allow developers to easily manage Spark jobs. |\r\n\r\n> <font color=\"red\">**Notes**</font>: \r\n>\r\n> 1. Current understanding of Spark Job Server and Client repos is preliminary and warrants further investigation.\r\n> \r\n> 2. It may make sense to merge the Scala Workbench and Spark Job Manager tools into a single application. \r\n\r\n## Tool Development\r\n\r\n### Scala Workbench\r\n\r\n* **Repo**: [zos-spark/scala-workbench](https://github.com/zos-spark/scala-workbench)\r\n* **Purpose**: A tightly-coupled Jupyter+Spark workbench that allows Scala users with direct access to transactional systems to query, analyze and visualize enterprise data.\r\n\r\n### Interactive Insights Workbench\r\n\r\n* **Repo**: [zos-spark/interactive-insights-workbench](https://github.com/zos-spark/interactive-insights-workbench)\r\n* **Purpose**: A [Jupyter Notebook](http://jupyter.org) based solution that enables the discovery of new insights through an interactive computing experience, which:\r\n\r\n\t* Mixes code, visualizations and annotations into a living document that can be shared for reproducibility\r\n\t* Promotes the ideation, exploration, collaboration and dissemination of information.\r\n\r\n\tThis general purpose instance of a Jupyter Notebook environment is intended as a baseline tool for data analysis, interactive visualization, numerical computing, and high-performance computing task against real-world datasets.\r\n\r\n* **Initial Scope**: Develop a docker recipe for easy recreation of a Jupyter Workbench that includes Python, R and dynamic dashboard capabilities. Minimum, requirements:\r\n\t* Support for Python, R and dynamic dashboards \r\n\t* [Baseline Recipe](https://github.com/jupyter/docker-stacks/wiki/Docker-Recipes#add-incubating-dashboard-declarative-widget-content-management-extensions)\r\n\t* [Bundling of notebooks as dashboards](https://github.com/jupyter-incubator/dashboards_bundlers) \r\n\t* Document the deployment/usage instructions for several deployment topologies (i.e., local, on-premise-cloud) \r\n\t* Provide sample notebooks (tutorials) that demonstrate how to connect to a NoSQL database using R and Python.\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}